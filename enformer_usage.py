# -*- coding: utf-8 -*-
"""

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/jaroddy/Altius_Enformer/blob/main/enformer/enformer-usage.ipynb

Copyright 2021 DeepMind Technologies Limited

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

     https://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

This colab showcases the usage of the Enformer model published in

**"Effective gene expression prediction from sequence by integrating long-range interactions"**

Å½iga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R. Ledsam, Agnieszka Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, David R. Kelley

###This is a modified version of Google's Enformer model for predicting gene expression from a supplied DNA sequence, or from the latest reference genome. It produces pooled(128 bases per value) and unpooled contribution scores for a region of one-hot encoded sequence 114688 bases in length, after being provided a region of 393216 bases in length. The modified program provides a list of transcription start sites that are within 50 kb of the interval. Please see the readme for more details on use.

"""### Imports"""

import os
import sys
#Mutes background dialogue concerning the lack of a GPU
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'
import tensorflow as tf
#import joblib
import tensorflow_hub as hub
import gzip
import kipoiseq
from kipoiseq import Interval
import pyfaidx
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns
from Bio import SeqIO
#Imports the custom one-hot encoder and fasta_extractor
import support_func as sf

#Paths
#transform_path = 'gs://dm-enformer/models/enformer.finetuned.SAD.robustscaler-PCA500-robustscaler.transform.pkl'
model_path = '/home/jrodriguez/altius_enformer/tf_models/c444fdff3e183daf686869692c26e00391f6773c'
fasta_file = 'genome.fa'
test_fasta = 'fasta_test.fasta'

# Download targets from Basenji2 dataset
# Cite: Kelley et al Cross-species regulatory sequence activity prediction. PLoS Comput. Biol. 16, e1008050 (2020).

targets_txt = 'targets_human.txt'
df_targets = pd.read_csv(targets_txt, sep='\t')
df_targets.head(3)

"""### Download files

Download and index the reference genome fasta file

Credit to Genome Reference Consortium: https://www.ncbi.nlm.nih.gov/grc

Schneider et al 2017 http://dx.doi.org/10.1101/gr.213611.116: Evaluation of GRCh38 and de novo haploid genome assemblies demonstrates the enduring quality of the reference assembly
"""

#Location of the reference genome: http://hgdownload.cse.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz

SEQUENCE_LENGTH = 393216

#Creating Enformer class including its functions

class Enformer:

  def __init__(self, tfhub_url):
    self._model = hub.load(tfhub_url).model
  #Actual predictions
  def predict_on_batch(self, inputs):
    predictions = self._model.predict_on_batch(inputs)
    return {k: v.numpy() for k, v in predictions.items()}
  
  
  @tf.function
  #Contribution Score calculation
  def contribution_input_grad(self, input_sequence,
                              target_mask, output_head='human'):
    #input_sequence = input_sequence[tf.newaxis]

    target_mask_mass = tf.reduce_sum(target_mask)
    with tf.GradientTape() as tape:
      tape.watch(input_sequence)
      prediction = tf.reduce_sum(
          target_mask[tf.newaxis] *
          self._model.predict_on_batch(input_sequence)[output_head]) / target_mask_mass

    input_grad = tape.gradient(prediction, input_sequence) * input_sequence
    input_grad = tf.squeeze(input_grad, axis=0)
    return tf.reduce_sum(input_grad, axis=-1)

#Vestigial Functions, may be integrated later   
class EnformerScoreVariantsRaw:

  def __init__(self, tfhub_url, organism='human'):
    self._model = Enformer(tfhub_url)
    self._organism = organism
  
  def predict_on_batch(self, inputs):
    ref_prediction = self._model.predict_on_batch(inputs['ref'])[self._organism]
    alt_prediction = self._model.predict_on_batch(inputs['alt'])[self._organism]

    return alt_prediction.mean(axis=1) - ref_prediction.mean(axis=1)


class EnformerScoreVariantsNormalized:

  def __init__(self, tfhub_url, transform_pkl_path,
               organism='human'):
    assert organism == 'human', 'Transforms only compatible with organism=human'
    self._model = EnformerScoreVariantsRaw(tfhub_url, organism)
    with tf.io.gfile.GFile(transform_pkl_path, 'rb') as f:
      transform_pipeline = joblib.load(f)
    self._transform = transform_pipeline.steps[0][1]  # StandardScaler.
    
  def predict_on_batch(self, inputs):
    scores = self._model.predict_on_batch(inputs)
    return self._transform.transform(scores)


class EnformerScoreVariantsPCANormalized:

  def __init__(self, tfhub_url, transform_pkl_path,
               organism='human', num_top_features=500):
    self._model = EnformerScoreVariantsRaw(tfhub_url, organism)
    with tf.io.gfile.GFile(transform_pkl_path, 'rb') as f:
      self._transform = joblib.load(f)
    self._num_top_features = num_top_features
    
  def predict_on_batch(self, inputs):
    scores = self._model.predict_on_batch(inputs)
    return self._transform.transform(scores)[:, :self._num_top_features]
#Original Fasta Extractor using pyfaidx
class FastaStringExtractor:
    
    def __init__(self, fasta_file):
        self.fasta = pyfaidx.Fasta(fasta_file)
        self._chromosome_sizes = {k: len(v) for k, v in self.fasta.items()}

    def extract(self, interval: Interval, **kwargs) -> str:
        # Truncate interval if it extends beyond the chromosome lengths.
        chromosome_length = self._chromosome_sizes[interval.chrom]
        trimmed_interval = Interval(interval.chrom,
                                    max(interval.start, 0),
                                    min(interval.end, chromosome_length),
                                    )
        # pyfaidx wants a 1-based interval
        sequence = str(self.fasta.get_seq(trimmed_interval.chrom,
                                          trimmed_interval.start + 1,
                                          trimmed_interval.stop).seq).upper()
        # Fill truncated values with N's.
        pad_upstream = 'N' * max(-interval.start, 0)
        pad_downstream = 'N' * max(interval.end - chromosome_length, 0)
        return pad_upstream + sequence + pad_downstream

    def close(self):
        return self.fasta.close()


def variant_generator(vcf_file, gzipped=False):
  """Yields a kipoiseq.dataclasses.Variant for each row in VCF file."""
  def _open(file):
    return gzip.open(vcf_file, 'rt') if gzipped else open(vcf_file)
    
  with _open(vcf_file) as f:
    for line in f:
      if line.startswith('#'):
        continue
      chrom, pos, id, ref, alt_list = line.split('\t')[:5]
      # Split ALT alleles and return individual variants as output.
      for alt in alt_list.split(','):
        yield kipoiseq.dataclasses.Variant(chrom=chrom, pos=pos,
                                           ref=ref, alt=alt, id=id)

#Kipoiseq one-hot encoder
def one_hot_encode(sequence):
  return kipoiseq.transforms.functional.one_hot_dna(sequence).astype(np.float32)


def variant_centered_sequences(vcf_file, sequence_length, gzipped=False,
                               chr_prefix=''):
  seq_extractor = kipoiseq.extractors.VariantSeqExtractor(
    reference_sequence=FastaStringExtractor(fasta_file))

  for variant in variant_generator(vcf_file, gzipped=gzipped):
    interval = Interval(chr_prefix + variant.chrom,
                        variant.pos, variant.pos)
    interval = interval.resize(sequence_length)
    center = interval.center() - interval.start

    reference = seq_extractor.extract(interval, [], anchor=center)
    alternate = seq_extractor.extract(interval, [variant], anchor=center)

    yield {'inputs': {'ref': one_hot_encode(reference),
                      'alt': one_hot_encode(alternate)},
           'metadata': {'chrom': chr_prefix + variant.chrom,
                        'pos': variant.pos,
                        'id': variant.id,
                        'ref': variant.ref,
                        'alt': variant.alt}}

#Function that finds local transcripts to a particular interval defined by chroms, bounds_lower, bounds_higher
def local_transcripts(transcripts, chroms, bounds_lower, bounds_higher):
   full_pos = pd.DataFrame()
   transcripts['Start'] = pd.to_numeric(transcripts['Start'])
   transcripts['End'] = pd.to_numeric(transcripts['End'])
   
   for x in range(len(chroms)):
       pos_temp = transcripts[transcripts['Seqid'] == chroms]
       pos_temp_plus = pos_temp[pos_temp['Strand'] == '+']
       pos_temp_negative = pos_temp[pos_temp['Strand'] =='-']
       pos_temp_plus = pos_temp_plus[pos_temp_plus['Start'] <= bounds_lower + 50000 +
                                                  (bounds_higher - bounds_lower)]
       pos_temp_plus = pos_temp_plus[pos_temp_plus['Start'] >= bounds_lower - 50000]
       pos_temp_negative = pos_temp_negative[pos_temp_negative['End'] <= bounds_higher + 50000]
       pos_temp_negative = pos_temp_negative[pos_temp_negative['End'] >= bounds_lower - 50000 - 
                                                  (bounds_higher-bounds_lower)]
       full_pos = full_pos.append(pos_temp_plus)
       full_pos = full_pos.append(pos_temp_negative)
   full_pos = full_pos.reset_index(drop=True)

   return full_pos
   

def plot_tracks(tracks, interval, height=1.5):
  fig, axes = plt.subplots(len(tracks), 1, figsize=(20, height * len(tracks)), sharex=True)
  for ax, (title, y) in zip(axes, tracks.items()):
    ax.fill_between(np.linspace(interval.start, interval.end, num=len(y)), y)
    ax.set_title(title)
    sns.despine(top=True, right=True, bottom=True)
  ax.set_xlabel(str(interval))
  plt.tight_layout()

"""## Make predictions for a genetic sequence"""
#Load transcripts
transcript = pd.read_csv('pos_report.csv')
#Load Regions of Interest
ROIs = pd.read_csv('ROIs.csv')
#Initialize the answer variable defining the type of analysis performed
question = 'X'
#Load the model
model = Enformer(model_path)

#Ask whether user is interested in testing variants (Y) or testing regions of interest from the reference genome (N)
while (question != 'Y') and (question != 'N'):

   msg = "Are you obtaining predictions for variants?"
   question = input("%s (Y/N) " % msg)

   if str(question) == 'Y':

      raw_sequences, handles = sf.fasta_extractor(test_fasta)

      one_hot = sf.one_hot_encoder(raw_sequences)

   elif str(question) == 'N':

      fasta_extractor = FastaStringExtractor(fasta_file)

   else:
   
      question = input("%s (Y/N) " % msg)



"""## Contribution scores calculation##"""
#If the user is only interested in the reference genome
if str(question) == 'N':
   #For each sample in the ROI.csv
   for x in range(len(ROIs)):

       print(x)
       #Temporary variables
       interval = str(ROIs['fasta_label'][x])

       begin = float(ROIs['bounds_lower'][x])

       end = float(ROIs['bounds_higher'][x])
       #Finding the local transcripts
       loc_trans = local_transcripts(transcript, interval, begin, end)  
       #Defining the target interval
       target_interval = kipoiseq.Interval(interval, int((begin-57344)), int((begin+57344)))  # @param
       #One-hot encode the relevant interval
       sequence_one_hot = one_hot_encode(fasta_extractor.extract(target_interval.resize(SEQUENCE_LENGTH)))
       #Make predictions for the target_mask
       predictions = model.predict_on_batch(sequence_one_hot[np.newaxis])['human'][0]
       
       target_mask = np.zeros_like(predictions)
       #For each index in the target_mask, set the track to 1
       for idx in [447, 448, 449]:
           #4760 corresponds to the CAGE: CD8+ Cells track (more can be found in targets_human.txt) 
           target_mask[idx, 4760] = 1
       
       sequence_one_hot = sequence_one_hot[tf.newaxis]  
       #Calculate Contribution scores
       contribution_scores = model.contribution_input_grad(sequence_one_hot.astype(np.float32), target_mask).numpy()
       #Calculate pooled scores
       pooled_contribution_scores = tf.nn.avg_pool1d(np.abs(contribution_scores)[np.newaxis, :, np.newaxis], 128, 128,
'VALID')[0, :, 0].numpy()[1088:-1088]
       print(pooled_contribution_scores)

       scores = pooled_contribution_scores
       #Save contribution scores
       np.savetxt('./predictions/' + "ROI_predictions_" + str(x) + ".csv", scores, delimiter=",")

#Similar functions to above but with variants       
elif str(question) == 'Y':
     #For variant one-hot encoded fasta files in the one_hot dictionary
     for y in range(len(one_hot)):

         predictions = model.predict_on_batch(one_hot[y])['human'][0]

         target_mask = np.zeros_like(predictions)

         for idx in [447, 448, 449]:

             target_mask[idx, 4760] = 1
          
         contribution_scores = model.contribution_input_grad(one_hot[y].astype(np.float32), target_mask).numpy()
         pooled_contribution_scores = tf.nn.avg_pool1d(np.abs(contribution_scores)[np.newaxis, :, np.newaxis], 128, 128, 'VALID')[0, :, 0].numpy()[1088:-1088]
         
         scores = pooled_contribution_scores
         #Save score files with fasta handle as the name
         np.savetxt('./predictions/' + str(handles[y]) + "_cont_scores.csv", scores, delimiter=",")





